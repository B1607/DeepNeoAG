{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271c45cf-e40c-417c-a83e-03b375add143",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbd76d2-6c8b-4901-896e-85c3a298082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:18:08.198218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 22:18:08.261979: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-10 22:18:08.606295: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64${LD_LIBRARY_PATH:+::/usr/local/cuda/lib64}\n",
      "2024-07-10 22:18:08.606394: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64${LD_LIBRARY_PATH:+::/usr/local/cuda/lib64}\n",
      "2024-07-10 22:18:08.606398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "import loading_data as load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e9da3-3fb9-4ed6-a197-380544b91cf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting of parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0851d5dd-21d4-4182-8fa5-2027de0091de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXSEQ = 35\n",
    "#The setting of sequence length.\n",
    "\n",
    "DATA_TYPE = \"prottrans\"\n",
    "#The type of data. Options are \"ProtTrans\", \"tape\", \"esm2\", \"esm1b\".\n",
    "\n",
    "NUM_FEATURE = 1024\n",
    "#\"The number of data feature dimensions. 1024 for ProtTrans, 768 for tape, 1280 for esm2 and esm1b.\"\n",
    "\n",
    "NUM_FILTER = 64\n",
    "#The number of filters in the convolutional layer.\n",
    "\n",
    "NUM_HIDDEN = 256\n",
    "#The number of hidden units in the dense layer.\n",
    "\n",
    "BATCH_SIZE  = 256\n",
    "#The batch size for training the model.\n",
    "\n",
    "WINDOW_SIZES = [4,8,16]\n",
    "#The window sizes for convolutional filters.\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = ['Negative','Positive']\n",
    "#The label of dataset.\n",
    "\n",
    "EPOCHS      = 50\n",
    "#The number of epochs for training the model.\n",
    "\n",
    "K_Fold = 5\n",
    "#The number of n-fold cross validation.\n",
    "\n",
    "VALIDATION_MODE=\"cross\"\n",
    "#The validation mode. Options are \"cross\", \"independent\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb44729-9da2-4f49-b84a-3f29e1fae1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCNN_MC\n",
      "\n",
      "The setting of sequence length:  35\n",
      "The number of filters in the convolutional layer:  64\n",
      "The number of hidden units in the dense layer:  256\n",
      "The batch size for training the model:  256\n",
      "The window sizes for convolutional filters:  [4, 8, 16]\n",
      "The validation mode:  cross\n",
      "The type of data:  prottrans\n",
      "The number of data feature dimensions:  1024\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMCNN_MC\\n\")\n",
    "print(\"The setting of sequence length: \",MAXSEQ)\n",
    "print(\"The number of filters in the convolutional layer: \",NUM_FILTER)\n",
    "print(\"The number of hidden units in the dense layer: \",NUM_HIDDEN)\n",
    "print(\"The batch size for training the model: \",BATCH_SIZE)\n",
    "print(\"The window sizes for convolutional filters: \",WINDOW_SIZES)\n",
    "print(\"The validation mode: \",VALIDATION_MODE)\n",
    "print(\"The type of data: \",DATA_TYPE)\n",
    "print(\"The number of data feature dimensions: \",NUM_FEATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf4caa-b045-4a63-8dab-cf66dd529db1",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1abe117-0b24-4a98-8f1e-57ad06c9e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit batch funtion\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, labels, batch_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        return np.array(batch_data), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad8130-4a7e-45ae-b873-d21cf32fd367",
   "metadata": {},
   "source": [
    "# MCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830c0e76-2144-4ea4-a3d3-c66e0c563930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepScan(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=[32],\n",
    "                 num_filters=256,\n",
    "                 num_hidden=1000):\n",
    "        # Initialize the parent class\n",
    "        super(DeepScan, self).__init__()\n",
    "        \n",
    "        # Initialize the input layer\n",
    "        self.input_layer = tf.keras.Input(input_shape)\n",
    "        \n",
    "        # Initialize convolution window sizes\n",
    "        self.window_sizes = window_sizes\n",
    "        \n",
    "        # Initialize lists to store convolution, pooling, and flatten layers\n",
    "        self.conv2d = []\n",
    "        self.maxpool = []\n",
    "        self.flatten = []\n",
    "        \n",
    "        # Create corresponding convolution, pooling, and flatten layers for each window size\n",
    "        for window_size in self.window_sizes:\n",
    "            self.conv2d.append(\n",
    "                layers.Conv2D(filters=num_filters,\n",
    "                              kernel_size=(1, window_size),\n",
    "                              activation=tf.nn.relu,\n",
    "                              padding='valid',\n",
    "                              bias_initializer=tf.constant_initializer(0.1),\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
    "            )\n",
    "            self.maxpool.append(\n",
    "                layers.MaxPooling2D(pool_size=(1, MAXSEQ - window_size + 1),\n",
    "                                    strides=(1, MAXSEQ),\n",
    "                                    padding='valid')\n",
    "            )\n",
    "            self.flatten.append(\n",
    "                layers.Flatten()\n",
    "            )\n",
    "        \n",
    "        # Initialize Dropout layer to prevent overfitting\n",
    "        self.dropout = layers.Dropout(rate=0.7)\n",
    "        \n",
    "        # Initialize the first fully connected layer\n",
    "        self.fc1 = layers.Dense(num_hidden,\n",
    "                                activation=tf.nn.relu,\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "        )\n",
    "        \n",
    "        # Initialize the output layer with softmax activation\n",
    "        self.fc2 = layers.Dense(NUM_CLASSES,\n",
    "                                activation='softmax',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "        \n",
    "        # Get the output layer by calling the call method\n",
    "        self.out = self.call(self.input_layer)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # List to store outputs of convolution, pooling, and flatten layers\n",
    "        _x = []\n",
    "        \n",
    "        # Perform convolution, pooling, and flatten operations for each window size\n",
    "        for i in range(len(self.window_sizes)):\n",
    "            x_conv = self.conv2d[i](x)\n",
    "            x_maxp = self.maxpool[i](x_conv)\n",
    "            x_flat = self.flatten[i](x_maxp)\n",
    "            _x.append(x_flat)\n",
    "        \n",
    "        # Concatenate the outputs of all flatten layers\n",
    "        x = tf.concat(_x, 1)\n",
    "        \n",
    "        # Apply Dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Apply the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd6e30-ad98-4e37-a76e-92cd7a39cfc6",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767da2b2-407b-45ab-8840-8056249aa086",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test= load_data.MCNN_data_load() #Load dataset from import_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569f495f-a64c-40e5-bbf2-fd8874023924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training dataset : (5608, 1, 35, 1024)\n",
      "The data type of training dataset : float16\n",
      "The shape of training label : (5608, 2)\n",
      "The shape of validation dataset : (1404, 1, 35, 1024)\n",
      "The data type of validation dataset : float16\n",
      "The shape of validation label : (1404, 2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of training dataset :\",x_train.shape)\n",
    "print(\"The data type of training dataset :\",x_train.dtype)\n",
    "print(\"The shape of training label :\",y_train.shape)\n",
    "print(\"The shape of validation dataset :\",x_test.shape)\n",
    "print(\"The data type of validation dataset :\",x_test.dtype)\n",
    "print(\"The shape of validation label :\",y_test.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14f0a3d-be25-4106-a4b1-a4bee9f1aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, x_test, y_test):\n",
    "    \n",
    "    # Generate predictions for the test data\n",
    "    pred_test = model.predict(x_test)\n",
    "    \n",
    "    # Calculate the false positive rate, true positive rate, and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:, 1], pred_test[:, 1])\n",
    "    # Calculate the Area Under the Curve (AUC) for the ROC curve\n",
    "    AUC = metrics.auc(fpr, tpr)\n",
    "    # Display the ROC curve\n",
    "    if (VALIDATION_MODE!=\"cross\"):\n",
    "        display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=AUC, estimator_name='mCNN')\n",
    "        display.plot()\n",
    "    \n",
    "    # Calculate the geometric mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    # Locate the index of the largest geometric mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print(f'\\nBest Threshold={thresholds[ix]}, G-Mean={gmeans[ix]}')\n",
    "    # Set the threshold to the one with the highest geometric mean\n",
    "    threshold = thresholds[ix]\n",
    "    # Generate binary predictions based on the threshold\n",
    "    y_pred = (pred_test[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix values: TN, FP, FN, TP\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y_test[:, 1], y_pred).ravel()\n",
    "    # Calculate Sensitivity (Recall)\n",
    "    Sens = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "    # Calculate Specificity\n",
    "    Spec = TN / (FP + TN) if FP + TN > 0 else 0.0\n",
    "    # Calculate Accuracy\n",
    "    Acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) if TP + FP > 0 and FP + TN > 0 and TP + FN and TN + FN else 0.0\n",
    "    # Calculate F1 Score\n",
    "    F1 = 2 * TP / (2 * TP + FP + FN)\n",
    "    # Calculate Precision\n",
    "    Prec = TP / (TP + FP)\n",
    "    # Calculate Recall\n",
    "    Recall = TP / (TP + FN)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(f'TP={TP}, FP={FP}, TN={TN}, FN={FN}, Sens={Sens:.4f}, Spec={Spec:.4f}, Acc={Acc:.4f}, MCC={MCC:.4f}, AUC={AUC:.4f}, F1={F1:.4f}, Prec={Prec:.4f}, Recall={Recall:.4f}\\n')\n",
    "    \n",
    "    # Return the performance metrics\n",
    "    return TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4314bd9a-10f4-40ef-a0b1-2d3788a6ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4486, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4486, 2)\n",
      "The shape of validation dataset of cross validation: (1122, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1122, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 1, 1, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 192)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:18:09.605632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.624901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.625009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.625879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.625984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.626059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.930726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.930863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.930946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:18:09.931010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5789 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:18:11.274861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8800\n",
      "2024-07-10 22:18:11.747191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/18 [==========>...................] - ETA: 0s - loss: 0.1462 - accuracy: 0.8940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:18:12.635933: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:18:12.635963: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:18:12.647367: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:18:12.647396: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:18:12.663266: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:18:12.663288: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 3s 40ms/step - loss: 1.0097 - accuracy: 0.9142\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2735 - accuracy: 0.9565\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2147 - accuracy: 0.9565\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2006 - accuracy: 0.9565\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1913 - accuracy: 0.9565\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2092 - accuracy: 0.9565\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1714 - accuracy: 0.9576\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1786 - accuracy: 0.9588\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1699 - accuracy: 0.9599\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1454 - accuracy: 0.9605\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1301 - accuracy: 0.9657\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2084 - accuracy: 0.9621\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1224 - accuracy: 0.9679\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1264 - accuracy: 0.9677\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1498 - accuracy: 0.9666\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1382 - accuracy: 0.9681\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0988 - accuracy: 0.9719\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1250 - accuracy: 0.9697\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1149 - accuracy: 0.9728\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1364 - accuracy: 0.9683\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0881 - accuracy: 0.9739\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1513 - accuracy: 0.9681\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0929 - accuracy: 0.9755\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1369 - accuracy: 0.9704\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0819 - accuracy: 0.9773\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0892 - accuracy: 0.9761\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0966 - accuracy: 0.9744\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0822 - accuracy: 0.9784\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0735 - accuracy: 0.9799\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0883 - accuracy: 0.9773\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0739 - accuracy: 0.9822\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1003 - accuracy: 0.9737\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0829 - accuracy: 0.9786\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0627 - accuracy: 0.9806\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0652 - accuracy: 0.9819\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0638 - accuracy: 0.9822\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0556 - accuracy: 0.9851\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0507 - accuracy: 0.9846\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0635 - accuracy: 0.9822\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0684 - accuracy: 0.9806\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0507 - accuracy: 0.9875\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0604 - accuracy: 0.9822\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0455 - accuracy: 0.9864\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0626 - accuracy: 0.9806\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0461 - accuracy: 0.9877\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0543 - accuracy: 0.9833\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0479 - accuracy: 0.9844\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0396 - accuracy: 0.9864\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0509 - accuracy: 0.9855\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0385 - accuracy: 0.9893\n",
      "36/36 [==============================] - 0s 5ms/step\n",
      "\n",
      "Best Threshold=3.337880116305314e-05, G-Mean=0.8873880618585519\n",
      "TP=42, FP=148, TN=928, FN=4, Sens=0.9130, Spec=0.8625, Acc=0.8645, MCC=0.4100, AUC=0.9333, F1=0.3559, Prec=0.2211, Recall=0.9130\n",
      "\n",
      "2 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4486, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4486, 2)\n",
      "The shape of validation dataset of cross validation: (1122, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1122, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 23ms/step - loss: 0.9198 - accuracy: 0.9024\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5849 - accuracy: 0.9574\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1964 - accuracy: 0.9574\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2537 - accuracy: 0.9574\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1717 - accuracy: 0.9576\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1505 - accuracy: 0.9576\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2163 - accuracy: 0.9574\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1329 - accuracy: 0.9608\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1311 - accuracy: 0.9634\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2276 - accuracy: 0.9594\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1227 - accuracy: 0.9630\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1267 - accuracy: 0.9672\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1542 - accuracy: 0.9654\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1570 - accuracy: 0.9654\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1872 - accuracy: 0.9648\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1133 - accuracy: 0.9692\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1115 - accuracy: 0.9706\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1129 - accuracy: 0.9695\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1052 - accuracy: 0.9699\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.1022 - accuracy: 0.9715\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0893 - accuracy: 0.9730\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1281 - accuracy: 0.9699\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1213 - accuracy: 0.9704\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1364 - accuracy: 0.9688\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1014 - accuracy: 0.9724\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0804 - accuracy: 0.9737\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0923 - accuracy: 0.9750\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0911 - accuracy: 0.9735\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0845 - accuracy: 0.9761\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1247 - accuracy: 0.9724\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.0744 - accuracy: 0.9764\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1025 - accuracy: 0.9724\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0670 - accuracy: 0.9782\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0600 - accuracy: 0.9811\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0929 - accuracy: 0.9753\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0674 - accuracy: 0.9793\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0641 - accuracy: 0.9819\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0585 - accuracy: 0.9819\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0603 - accuracy: 0.9813\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0659 - accuracy: 0.9795\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0586 - accuracy: 0.9819\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0522 - accuracy: 0.9833\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0606 - accuracy: 0.9817\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0493 - accuracy: 0.9860\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0453 - accuracy: 0.9875\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0384 - accuracy: 0.9889\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0579 - accuracy: 0.9817\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0535 - accuracy: 0.9842\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0351 - accuracy: 0.9897\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0511 - accuracy: 0.9846\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "\n",
      "Best Threshold=0.00031986527028493583, G-Mean=0.8344271428449689\n",
      "TP=40, FP=139, TN=933, FN=10, Sens=0.8000, Spec=0.8703, Acc=0.8672, MCC=0.3777, AUC=0.8958, F1=0.3493, Prec=0.2235, Recall=0.8000\n",
      "\n",
      "3 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4486, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4486, 2)\n",
      "The shape of validation dataset of cross validation: (1122, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1122, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 24ms/step - loss: 0.2848 - accuracy: 0.8651\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.7596 - accuracy: 0.9561\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4136 - accuracy: 0.9561\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1942 - accuracy: 0.9561\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1829 - accuracy: 0.9561\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1638 - accuracy: 0.9563\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1945 - accuracy: 0.9561\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1519 - accuracy: 0.9581\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1645 - accuracy: 0.9588\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1546 - accuracy: 0.9610\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1881 - accuracy: 0.9610\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1521 - accuracy: 0.9650\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1572 - accuracy: 0.9643\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1342 - accuracy: 0.9659\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1474 - accuracy: 0.9677\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1466 - accuracy: 0.9672\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2165 - accuracy: 0.9632\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1178 - accuracy: 0.9704\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1309 - accuracy: 0.9690\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1190 - accuracy: 0.9701\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1069 - accuracy: 0.9692\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1095 - accuracy: 0.9690\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0951 - accuracy: 0.9730\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1006 - accuracy: 0.9737\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1163 - accuracy: 0.9695\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1021 - accuracy: 0.9719\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.1010 - accuracy: 0.9726\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1205 - accuracy: 0.9719\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0876 - accuracy: 0.9748\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1064 - accuracy: 0.9721\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0816 - accuracy: 0.9757\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0770 - accuracy: 0.9766\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0882 - accuracy: 0.9750\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0872 - accuracy: 0.9748\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0809 - accuracy: 0.9755\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0701 - accuracy: 0.9802\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0830 - accuracy: 0.9761\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0662 - accuracy: 0.9815\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0833 - accuracy: 0.9766\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0804 - accuracy: 0.9761\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0619 - accuracy: 0.9826\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0714 - accuracy: 0.9779\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0539 - accuracy: 0.9853\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0957 - accuracy: 0.9757\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0612 - accuracy: 0.9811\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0829 - accuracy: 0.9755\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0611 - accuracy: 0.9822\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0600 - accuracy: 0.9813\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0497 - accuracy: 0.9844\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0511 - accuracy: 0.9851\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "\n",
      "Best Threshold=0.0007891328423283994, G-Mean=0.8257228238447705\n",
      "TP=35, FP=154, TN=924, FN=9, Sens=0.7955, Spec=0.8571, Acc=0.8547, MCC=0.3385, AUC=0.8772, F1=0.3004, Prec=0.1852, Recall=0.7955\n",
      "\n",
      "4 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4487, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4487, 2)\n",
      "The shape of validation dataset of cross validation: (1121, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1121, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      " 1/18 [>.............................] - ETA: 14s - loss: 0.5702 - accuracy: 0.8008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:19:25.836634: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:19:25.836662: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:19:25.848399: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-07-10 22:19:25.848439: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 40ms/step - loss: 1.1414 - accuracy: 0.9450\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.3938 - accuracy: 0.9563\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2272 - accuracy: 0.9563\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.2872 - accuracy: 0.9563\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2208 - accuracy: 0.9563\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2382 - accuracy: 0.9565\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1853 - accuracy: 0.9592\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1352 - accuracy: 0.9641\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1722 - accuracy: 0.9601\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1903 - accuracy: 0.9603\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1939 - accuracy: 0.9626\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2368 - accuracy: 0.9623\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1430 - accuracy: 0.9672\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1455 - accuracy: 0.9670\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1288 - accuracy: 0.9681\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1382 - accuracy: 0.9681\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1228 - accuracy: 0.9677\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1124 - accuracy: 0.9710\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1171 - accuracy: 0.9688\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1560 - accuracy: 0.9675\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1533 - accuracy: 0.9637\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.2087 - accuracy: 0.9677\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1175 - accuracy: 0.9710\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1252 - accuracy: 0.9717\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1083 - accuracy: 0.9715\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0984 - accuracy: 0.9730\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1156 - accuracy: 0.9701\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0918 - accuracy: 0.9739\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0899 - accuracy: 0.9759\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1099 - accuracy: 0.9715\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0823 - accuracy: 0.9762\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0939 - accuracy: 0.9746\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0808 - accuracy: 0.9770\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0805 - accuracy: 0.9764\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0697 - accuracy: 0.9793\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1062 - accuracy: 0.9733\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1160 - accuracy: 0.9724\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0955 - accuracy: 0.9750\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0750 - accuracy: 0.9766\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0854 - accuracy: 0.9755\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0687 - accuracy: 0.9817\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0882 - accuracy: 0.9748\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0620 - accuracy: 0.9831\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0792 - accuracy: 0.9799\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0661 - accuracy: 0.9806\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0586 - accuracy: 0.9824\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0523 - accuracy: 0.9835\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0661 - accuracy: 0.9806\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0487 - accuracy: 0.9848\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0448 - accuracy: 0.9869\n",
      "36/36 [==============================] - 0s 5ms/step\n",
      "\n",
      "Best Threshold=0.0008240513852797449, G-Mean=0.8827831397750683\n",
      "TP=38, FP=83, TN=993, FN=7, Sens=0.8444, Spec=0.9229, Acc=0.9197, MCC=0.4854, AUC=0.9463, F1=0.4578, Prec=0.3140, Recall=0.8444\n",
      "\n",
      "5 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4487, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4487, 2)\n",
      "The shape of validation dataset of cross validation: (1121, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1121, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 23ms/step - loss: 0.3782 - accuracy: 0.9572\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2948 - accuracy: 0.9588\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2041 - accuracy: 0.9588\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2122 - accuracy: 0.9588\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2439 - accuracy: 0.9590\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1698 - accuracy: 0.9610\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.1714 - accuracy: 0.9608\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1358 - accuracy: 0.9650\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.1408 - accuracy: 0.9652\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1174 - accuracy: 0.9681\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1203 - accuracy: 0.9688\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1211 - accuracy: 0.9699\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1202 - accuracy: 0.9704\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.1007 - accuracy: 0.9735\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1011 - accuracy: 0.9737\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1189 - accuracy: 0.9717\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0870 - accuracy: 0.9779\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.1037 - accuracy: 0.9737\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1013 - accuracy: 0.9750\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0874 - accuracy: 0.9762\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.1140 - accuracy: 0.9726\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0760 - accuracy: 0.9799\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.1084 - accuracy: 0.9735\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0782 - accuracy: 0.9802\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0652 - accuracy: 0.9824\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0599 - accuracy: 0.9837\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0617 - accuracy: 0.9824\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0557 - accuracy: 0.9866\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0491 - accuracy: 0.9871\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0736 - accuracy: 0.9804\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0544 - accuracy: 0.9882\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0510 - accuracy: 0.9862\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0486 - accuracy: 0.9864\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0447 - accuracy: 0.9882\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0454 - accuracy: 0.9889\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0411 - accuracy: 0.9886\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0351 - accuracy: 0.9926\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0436 - accuracy: 0.9886\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0464 - accuracy: 0.9860\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.0324 - accuracy: 0.9915\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0356 - accuracy: 0.9895\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0256 - accuracy: 0.9933\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0452 - accuracy: 0.9860\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0367 - accuracy: 0.9902\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0323 - accuracy: 0.9911\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0296 - accuracy: 0.9913\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0400 - accuracy: 0.9886\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0314 - accuracy: 0.9920\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.0299 - accuracy: 0.9911\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.0286 - accuracy: 0.9940\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "\n",
      "Best Threshold=0.0001679857523413375, G-Mean=0.858392505106847\n",
      "TP=47, FP=130, TN=935, FN=9, Sens=0.8393, Spec=0.8779, Acc=0.8760, MCC=0.4285, AUC=0.9068, F1=0.4034, Prec=0.2655, Recall=0.8393\n",
      "\n",
      "The mean of 5-Fold cross-validation results:\n",
      "TP=40.4, FP=130.8, TN=942.6, FN=7.8, Sens=0.8384, Spec=0.8781, Acc=0.8764, MCC=0.408, AUC=0.9119, F1=0.3734, Prec=0.2419, Recall=0.2419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if(VALIDATION_MODE == \"cross\"):\n",
    "    # Initialize K-Fold cross-validation\n",
    "    kfold = KFold(n_splits=K_Fold, shuffle=True, random_state=2)\n",
    "    \n",
    "    results = []  # List to store results of each fold\n",
    "    i = 1  # Counter for fold number\n",
    "    \n",
    "    # Iterate over each split of the dataset\n",
    "    for train_index, test_index in kfold.split(x_train):\n",
    "        print(f\"{i} / {K_Fold}\\n\")\n",
    "        \n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "        Y_train, Y_test = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        # Print the shapes of the training and testing datasets\n",
    "        print(\"The shape of training dataset of cross validation:\", X_train.shape)\n",
    "        print(\"The shape of training label of cross validation:\", Y_train.shape)\n",
    "        print(\"The shape of validation dataset of cross validation:\", X_test.shape)\n",
    "        print(\"The shape of validation label of cross validation:\", Y_test.shape)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Create a data generator for the training data\n",
    "        generator = DataGenerator(X_train, Y_train, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Initialize the DeepScan model\n",
    "        model = DeepScan(\n",
    "            num_filters=NUM_FILTER,\n",
    "            num_hidden=NUM_HIDDEN,\n",
    "            window_sizes=WINDOW_SIZES\n",
    "        )\n",
    "        \n",
    "        # Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Build the model with the input shape of the training data\n",
    "        model.build(input_shape=X_train.shape)\n",
    "        \n",
    "        # Print the model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            generator,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)],\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Test the model on the validation set and get performance metrics\n",
    "        TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall = model_test(model, X_test, Y_test)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        results.append([TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall])\n",
    "        \n",
    "        # Increment the fold counter\n",
    "        i += 1\n",
    "        \n",
    "        # Clear the training and testing data from memory\n",
    "        del X_train\n",
    "        del X_test\n",
    "        del Y_train\n",
    "        del Y_test\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate the mean results across all folds\n",
    "    mean_results = np.mean(results, axis=0)\n",
    "    \n",
    "    # Print the mean results of the cross-validation\n",
    "    print(f\"The mean of {K_Fold}-Fold cross-validation results:\")\n",
    "    print(f'TP={mean_results[0]:.4}, FP={mean_results[1]:.4}, TN={mean_results[2]:.4}, FN={mean_results[3]:.4}, '\n",
    "          f'Sens={mean_results[4]:.4}, Spec={mean_results[5]:.4}, Acc={mean_results[6]:.4}, MCC={mean_results[7]:.4}, AUC={mean_results[8]:.4}, F1={mean_results[9]:.4}, Prec={mean_results[10]:.4}, Recall={mean_results[10]:.4}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28aa9cbe-3dc7-41a0-af37-8b2baffd372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(VALIDATION_MODE == \"independent\"):\n",
    "    # Create a data generator for the training data\n",
    "    generator = DataGenerator(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize the DeepScan model\n",
    "    model = DeepScan(\n",
    "        num_filters=NUM_FILTER,\n",
    "        num_hidden=NUM_HIDDEN,\n",
    "        window_sizes=WINDOW_SIZES\n",
    "    )\n",
    "    \n",
    "    # Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Build the model with the input shape of the training data\n",
    "    model.build(input_shape=x_train.shape)\n",
    "    \n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        generator,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    # Test the model on the independent test set and get performance metrics\n",
    "    TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall = model_test(model, x_test, y_test)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(f'TP={TP}, FP={FP}, TN={TN}, FN={FN}, Sens={Sens:.4f}, Spec={Spec:.4f}, Acc={Acc:.4f}, MCC={MCC:.4f}, AUC={AUC:.4f}, F1={F1:.4f}, Prec={Prec:.4f}, Recall={Recall:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfa560-7be0-4d44-a4e4-0e01f591be4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
